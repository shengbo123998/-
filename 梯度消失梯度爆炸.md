<img width="824" alt="image" src="https://github.com/user-attachments/assets/673471f1-02ae-4c46-805b-ba7bc49de2e2" />https://www.cnblogs.com/emanlee/p/17068162.html 
梯度消失原因：
1. 激活函数，最大值1/4，在边缘的梯度更低
   <img width="824" alt="image" src="https://github.com/user-attachments/assets/aab69a91-8eb0-4df0-a8f2-6ebdc9cea40d" />
2. 权重初始化比较小
3. 网络层数太深

梯度爆炸的原因：
1. 权重初始化
2. 学习率太高
3. 网络层数太深

梯度消失解决方法：

  1. sigmoid激活函数的导数最大值为1/4，再乘w, 容易产生梯度消失
  解决： 换成relu,leakyrelu,ELU

  2. BN层可以将数据分布转化成正态分布，使数据落在sigmoid梯度比较大的中间区域

  3.    残差结构与LSTM可以有效防止梯度消失

梯度爆炸原因及其解决：
   1.BN
   2. w权重过大，导致计算的梯度过大，权重正则化
   3. 梯度截断
   4. 学习率过大，降低学习率
<img width="916" alt="image" src="https://github.com/user-attachments/assets/b836fc66-a7dd-40f3-bdf3-c14cb5b3884d" />


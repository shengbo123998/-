https://www.zhihu.com/tardis/zm/art/142958834?source_id=1003

为什么在Google的Wide&Deep模型中，要使用带L1正则化项的FTRL作为wide部分的优化方法，而使用AdaGrad作为deep部分的优化方法？
wide主要用于稀疏特征，且是因果关系比较直接的，原文中使用了用户安装过的app,跟曝光的app，乘积，输入到wide， 因为用户安装过的app跟曝光app相似的话，跟会不会安装当前曝光app有很大的关系。
所以wide的记忆性是为了保留更直接的因果关系，做稀疏特征的交叉。而deep直接学稀疏特征，很难学好泛化性
